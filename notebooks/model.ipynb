{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5940656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚢 Training Titanic Survival Prediction Models...\n",
      "✅ All models trained and saved!\n",
      "📊 Random Forest Test Accuracy: 0.7821\n",
      "📊 Logistic Regression Test Accuracy: 0.8045\n",
      "📊 Gradient Boosting Test Accuracy: 0.7709\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def comprehensive_feature_engineering(data):\n",
    "    \"\"\"\n",
    "    Fixed feature engineering that handles single-row predictions\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Extract titles\n",
    "    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\\\\\\\.', expand=False)\n",
    "    title_mapping = {\n",
    "        'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',\n",
    "        'Rev': 'Officer', 'Dr': 'Officer', 'Col': 'Officer', 'Major': 'Officer',\n",
    "        'Capt': 'Officer', 'Don': 'Royalty', 'Dona': 'Royalty', 'Sir': 'Royalty',\n",
    "        'Lady': 'Royalty', 'Countess': 'Royalty', 'Jonkheer': 'Royalty'\n",
    "    }\n",
    "    df['Title_Grouped'] = df['Title'].map(title_mapping).fillna('Other')\n",
    "    \n",
    "    # Handle missing values\n",
    "    age_by_title = df.groupby('Title_Grouped')['Age'].median()\n",
    "    for title in age_by_title.index:\n",
    "        mask = (df['Age'].isnull()) & (df['Title_Grouped'] == title)\n",
    "        df.loc[mask, 'Age'] = age_by_title[title]\n",
    "    \n",
    "    df['Age'] = df['Age'].fillna(df['Age'].median())\n",
    "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "    \n",
    "    # Create features\n",
    "    df['Family_Size'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['Is_Alone'] = (df['Family_Size'] == 1).astype(int)\n",
    "    df['Small_Family'] = (df['Family_Size'].between(2, 4)).astype(int)\n",
    "    df['Large_Family'] = (df['Family_Size'] > 4).astype(int)\n",
    "    df['Fare_Per_Person'] = df['Fare'] / df['Family_Size']\n",
    "    \n",
    "    # 🔧 FIXED: Handle fare binning for single predictions\n",
    "    try:\n",
    "        # Try quantile-based binning first\n",
    "        df['Fare_Bin'] = pd.qcut(df['Fare'], 4, labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "    except ValueError:\n",
    "        # If that fails (single value), use manual bins based on historical data\n",
    "        df['Fare_Bin'] = pd.cut(df['Fare'], \n",
    "                               bins=[0, 7.91, 14.454, 31, np.inf], \n",
    "                               labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "    \n",
    "    # Age bins\n",
    "    df['Age_Bin'] = pd.cut(df['Age'], \n",
    "                          bins=[0, 12, 18, 35, 60, 100], \n",
    "                          labels=['Child', 'Teen', 'Adult', 'Middle', 'Senior'])\n",
    "    df['Is_Child'] = (df['Age'] < 16).astype(int)\n",
    "    \n",
    "    # 🔧 FIXED: Handle cabin extraction properly\n",
    "    df['Has_Cabin'] = df['Cabin'].notna().astype(int)\n",
    "    df['Cabin'] = df['Cabin'].fillna('Unknown')\n",
    "    df['Deck'] = df['Cabin'].astype(str).str[0]\n",
    "    df['Ticket_Len'] = df['Ticket'].astype(str).apply(len)\n",
    "    \n",
    "    # Interaction features\n",
    "    df['Age_Class'] = df['Age'] * df['Pclass']\n",
    "    df['Fare_Class'] = df['Fare'] / df['Pclass']\n",
    "    df['Family_Fare'] = df['Family_Size'] * df['Fare']\n",
    "    \n",
    "    # Encoding\n",
    "    df['Sex_binary'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "    embarked_mapping = {'S': 0, 'Q': 1, 'C': 2}\n",
    "    df['Embarked_ordinal'] = df['Embarked'].map(embarked_mapping)\n",
    "    \n",
    "    # One-hot encoding\n",
    "    df = pd.get_dummies(df, columns=['Title_Grouped', 'Fare_Bin', 'Age_Bin', 'Deck'], prefix_sep='_')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Train and save models\n",
    "def train_and_save_models():\n",
    "    \"\"\"Train and save all three models\"\"\"\n",
    "    \n",
    "    print(\"🚢 Training Titanic Survival Prediction Models...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv('C:\\\\Users\\\\prana\\\\Desktop\\\\Uni\\\\Sem 5\\\\AI_Hackathon\\\\threeD_Titanic_Classification\\\\data\\\\Titanic-Dataset.csv')\n",
    "    \n",
    "    # Apply feature engineering\n",
    "    df_engineered = comprehensive_feature_engineering(df)\n",
    "    \n",
    "    # Define features\n",
    "    numeric_base_features = ['Pclass', 'Sex_binary', 'Age', 'Fare', 'Family_Size', 'Is_Alone', \n",
    "                            'Has_Cabin', 'Embarked_ordinal', 'Age_Class', 'Fare_Class', \n",
    "                            'Family_Fare', 'Is_Child', 'Small_Family', 'Large_Family', \n",
    "                            'Fare_Per_Person', 'Ticket_Len']\n",
    "    \n",
    "    onehot_features = [col for col in df_engineered.columns if col.startswith(\n",
    "        ('Title_Grouped_', 'Fare_Bin_', 'Age_Bin_', 'Deck_'))]\n",
    "    \n",
    "    feature_columns = numeric_base_features + onehot_features\n",
    "    \n",
    "    X = df_engineered[feature_columns]\n",
    "    y = df_engineered['Survived']\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # 1. Random Forest\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    rf_data = {\n",
    "        'model': rf_model,\n",
    "        'feature_columns': feature_columns\n",
    "    }\n",
    "    \n",
    "    with open('titanic_rf_model.pkl', 'wb') as f:\n",
    "        pickle.dump(rf_data, f)\n",
    "    \n",
    "    # 2. Logistic Regression (with scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    lr_data = {\n",
    "        'model': lr_model,\n",
    "        'scaler': scaler,\n",
    "        'feature_columns': feature_columns\n",
    "    }\n",
    "    \n",
    "    with open('titanic_lr_model.pkl', 'wb') as f:\n",
    "        pickle.dump(lr_data, f)\n",
    "    \n",
    "    # 3. Gradient Boosting\n",
    "    gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    \n",
    "    gb_data = {\n",
    "        'model': gb_model,\n",
    "        'feature_columns': feature_columns\n",
    "    }\n",
    "    \n",
    "    with open('titanic_gb_model.pkl', 'wb') as f:\n",
    "        pickle.dump(gb_data, f)\n",
    "    \n",
    "    print(f\"✅ All models trained and saved!\")\n",
    "    print(f\"📊 Random Forest Test Accuracy: {rf_model.score(X_test, y_test):.4f}\")\n",
    "    print(f\"📊 Logistic Regression Test Accuracy: {lr_model.score(X_test_scaled, y_test):.4f}\")\n",
    "    print(f\"📊 Gradient Boosting Test Accuracy: {gb_model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "def predict_from_form(pclass, gender, age, sibsp, parch, fare, embarked, model_type='Random Forest'):\n",
    "    \"\"\"\n",
    "    🎯 MAIN PREDICTION FUNCTION FOR YOUR FORM\n",
    "    \n",
    "    Parameters (exactly matching your form):\n",
    "    - pclass: 1, 2, or 3\n",
    "    - gender: 'Male' or 'Female'\n",
    "    - age: float\n",
    "    - sibsp: int (Number of Siblings/Spouses)\n",
    "    - parch: int (Number of Parents/Children)\n",
    "    - fare: float (Fare Paid)\n",
    "    - embarked: 'Southampton', 'Cherbourg', or 'Queenstown'\n",
    "    - model_type: 'Random Forest', 'Logistic Regression', or 'Gradient Boosting'\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load the selected model\n",
    "        model_files = {\n",
    "            'Random Forest': 'titanic_rf_model.pkl',\n",
    "            'Logistic Regression': 'titanic_lr_model.pkl', \n",
    "            'Gradient Boosting': 'titanic_gb_model.pkl'\n",
    "        }\n",
    "        \n",
    "        with open(model_files[model_type], 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Convert form inputs\n",
    "        embarked_map = {'Southampton': 'S', 'Cherbourg': 'C', 'Queenstown': 'Q'}\n",
    "        embarked_code = embarked_map[embarked]\n",
    "        \n",
    "        # Create input dataframe\n",
    "        input_data = pd.DataFrame({\n",
    "            'Pclass': [pclass],\n",
    "            'Sex': [gender.lower()],\n",
    "            'Age': [age],\n",
    "            'SibSp': [sibsp],\n",
    "            'Parch': [parch],\n",
    "            'Fare': [fare],\n",
    "            'Embarked': [embarked_code],\n",
    "            'Name': ['User, Mr. Test'],\n",
    "            'Cabin': [np.nan],\n",
    "            'Ticket': ['12345']\n",
    "        })\n",
    "        \n",
    "        # Apply feature engineering\n",
    "        processed_data = comprehensive_feature_engineering(input_data)\n",
    "        \n",
    "        # Get model components\n",
    "        model = model_data['model']\n",
    "        feature_columns = model_data['feature_columns']\n",
    "        \n",
    "        # Ensure all feature columns exist\n",
    "        for col in feature_columns:\n",
    "            if col not in processed_data.columns:\n",
    "                processed_data[col] = 0\n",
    "        \n",
    "        # Select features in correct order\n",
    "        X_input = processed_data[feature_columns].astype(float)\n",
    "        \n",
    "        # Apply scaling for Logistic Regression\n",
    "        if model_type == 'Logistic Regression':\n",
    "            scaler = model_data['scaler']\n",
    "            X_input = scaler.transform(X_input)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(X_input)[0]\n",
    "        probability = model.predict_proba(X_input)[0][1]  # Probability of survival\n",
    "        \n",
    "        return {\n",
    "            'model_used': model_type,\n",
    "            'survival_prediction': 'Survived' if prediction == 1 else 'Not Survived',\n",
    "            'survival_probability': round(probability, 3),\n",
    "            'death_probability': round(1 - probability, 3),\n",
    "            'emoji': '✅' if prediction == 1 else '☠️'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': f\"Prediction failed: {str(e)}\",\n",
    "            'survival_prediction': 'Error',\n",
    "            'survival_probability': 0.0\n",
    "        }\n",
    "\n",
    "# Train models first (run this once)\n",
    "train_and_save_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
